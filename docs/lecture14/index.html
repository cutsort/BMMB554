<!doctype html>
<html lang="en" class="h-100">
  <head>
  <meta name="generator" content="Hugo 0.54.0" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <link rel="stylesheet" href="https://nekrut.github.io/BMMB554/css/bootstrap.min.css">
  
  
  <title>Lecture 14: Intro into discrete data | BMMB554</title>
  <style>
.container {
  max-width: 800px;
}
#nav a {
  font-weight: bold;
  color: inherit;
}
#nav a.nav-link-active {
  background-color: #212529;
  color: #fff;
}
#nav-border {
  border-bottom: 1px solid #212529;
}
#main {
  margin-top: 1em;
  margin-bottom: 4em;
}
#home-jumbotron {
  background-color: inherit;
}
#footer .container {
  padding: 1em 0;
}
#footer a {
  color: inherit;
  text-decoration: underline;
}
.font-125 {
  font-size: 125%;
}
.tag-btn {
  margin-bottom: 0.3em;
}
pre {
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  padding: 16px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  background-color: transparent;
  border-radius: 0;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 4px;
}
img {
  max-width: 100%;
}
</style>

  <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
  
</head>
  <body class="d-flex flex-column h-100">
    <div id="nav-border" class="container">
  <nav id="nav" class="nav justify-content-center">
  
  
  
    
    
      
        
      
    
    
    <a class="nav-link " href="/BMMB554/"><i data-feather="home"></i> Home</a>
  
    
    
      
        
      
    
    
    <a class="nav-link " href="/BMMB554/post/"><i data-feather="book-open"></i> Lectures</a>
  
    
    
      
        
      
    
    
    <a class="nav-link " href="/BMMB554/tags/"><i data-feather="tag"></i> Tags</a>
  
    
    
      
        
      
    
    
    <a class="nav-link " href="/BMMB554/about/"><i data-feather="list"></i> Syllabus</a>
  
  </nav>
</div>
    <div class="container">
      <main id="main">
        

<h1>Lecture 14: Intro into discrete data</h1>


<i data-feather="calendar"></i> <time datetime="2019-03-18">Mar 18, 2019</time>

  <br>
  <i data-feather="tag"></i>
  
  
  <a class="btn btn-sm btn-outline-dark tag-btn" href="https://nekrut.github.io/BMMB554/tags/assembly">assembly</a>
  
  
  <a class="btn btn-sm btn-outline-dark tag-btn" href="https://nekrut.github.io/BMMB554/tags/debruijn">DeBruijn</a>
  

<br><br>


<p><a href="https://colab.research.google.com/github/nekrut/msmb_python/blob/master/chapter1.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>

<div class="alert alert-info" role="alert">
  This material is based on <a href="http://web.stanford.edu/class/bios221/book/">Modern Statistics for Modern Biology</a> by Susan Holmes and Wolfgang Huber. It was translated into Python and adopted to Jupyter environment by AN.
</div>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#408080;font-style:italic"># Make sure we have latest seaborn</span>
<span style="">!</span>pip install seaborn <span style="color:#666">--</span>upgrade</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#408080;font-style:italic"># Make sure we have latest numpy</span>
<span style="">!</span>pip install numpy <span style="color:#666">--</span>upgrade</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#008000;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">numpy</span> <span style="color:#008000;font-weight:bold">as</span> <span style="color:#00f;font-weight:bold">np</span>
<span style="color:#008000;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">pandas</span> <span style="color:#008000;font-weight:bold">as</span> <span style="color:#00f;font-weight:bold">pd</span>
<span style="color:#008000;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">IPython.display</span> <span style="color:#008000;font-weight:bold">import</span> Image
<span style="color:#008000;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">matplotlib.pyplot</span> <span style="color:#008000;font-weight:bold">as</span> <span style="color:#00f;font-weight:bold">plt</span>
<span style="color:#008000;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">matplotlib.ticker</span> <span style="color:#008000;font-weight:bold">as</span> <span style="color:#00f;font-weight:bold">ticker</span></code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#008000;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">seaborn</span> <span style="color:#008000;font-weight:bold">as</span> <span style="color:#00f;font-weight:bold">sns</span>
sns<span style="color:#666">.</span>set_context(<span style="color:#ba2121">&#34;notebook&#34;</span>, font_scale<span style="color:#666">=</span><span style="color:#666">1.5</span>, rc<span style="color:#666">=</span>{<span style="color:#ba2121">&#34;lines.linewidth&#34;</span>: <span style="color:#666">2.5</span>})</code></pre></div>
<h1 id="0-statistical-distribution-in-python">0. Statistical distribution in Python</h1>

<h2 id="0-1-random-variables">0.1. Random variables</h2>

<p>A <em>random</em> variable $X$ is a variable with values representing outcomes of a random process. There are two kinds of random variables: <strong>discrete</strong> and <strong>continuous</strong>.</p>

<p>A <strong><a href="https://en.wikipedia.org/wiki/Random_variable#Discrete_random_variable">discrete</a></strong> random variable takes a finite number of distinct values. For example, the number of reads mapping at a position in the genome is an example of discrete random variable.</p>

<p>The probability distribution (<em>probability function</em> or <em>probability mass function</em>) of a discrete random variable is a list of probabilities associated with each value. For a random variable $X$ that can take $k$ values:</p>

<p>$P(X=x_i)=p_i, where\ i\ \in (0; k]$</p>

<p>This requires:</p>

<p>$(1)\ \ \ 0 &lt; p_i &lt; 1 $ <br />
$(2) \ \ \  p_1+p_2+&hellip;+p_k=1$</p>

<p>In today&rsquo;s lecture we will use exclusively discrete probability distributions.</p>

<p>A <strong><a href="https://en.wikipedia.org/wiki/Random_variable#Continuous_random_variable">continuous</a></strong> random variable can have infinite number of possible values. The tempereature in State College throughout the year is an example of a continuous random variable. Because the continuous random variable is defined over an interval of values, it is represented by the area under a curve (an integral).</p>

<p>The probability of any given outcome is 0, but the probability that $X$ takes a set of values within an interval is defined as the area under the curve defined by the probability distribution function. This curve must satisfy:</p>

<p>$(1)\ \ \ p(x) &gt; 0\ for\ all\ x$ <br />
$(2) \ \ \ The\ total\ area\ under\ this\ curve\ is\ equal\ to\ 1$</p>

<h2 id="0-2-distributions-in-scipy">0.2. Distributions in <code>SciPy</code></h2>

<p>The Scipy module includes a larger list of random variate generators including over 80 continuous and 10 discrete random variable distributions. For each
distribution, a number of functions are available:</p>

<ul>
<li><code>rvs</code>: random variable generator</li>
<li><code>pdf</code>: probability density function (or <code>pmf</code>, probaility mass function)</li>
<li><code>cdf</code>: cumulative distribution function</li>
<li><code>sf</code>: survival function (1-<code>cdf</code>)</li>
<li><code>ppf</code>: percent point function (Inverse )</li>
<li><code>isf</code>: Inverse Survival Function (Inverse of SF)</li>
<li><code>stats</code>: Return mean, variance, (Fisher’s) skew, or (Fisher’s) kurtosis</li>
<li><code>moment</code>: non-central moments of the distribution</li>
</ul>

<h2 id="0-3-a-bionomial-example">0.3. A bionomial example</h2>

<p>Binomial distribution is defined on a finite set consisting of all the possible results of $N$ tries of an experiment with a binary outcome, $0$ or $1$. If $p$ is the probability of getting a $1$ and $1-p$ that of getting a $0$, the probability that $k$ out of the $N$ tries yield a $1$ is:</p>

<p>$P(k\ 1s\ out\ of\ N) = \binom{N}{k}p^k(1-p)^{N-k}$</p>

<p>it mean is $\mu = Np$ and variance is $\sigma^2=Np(1-p)$</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#008000;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">scipy.stats</span> <span style="color:#008000;font-weight:bold">import</span> binom
b <span style="color:#666">=</span> binom<span style="color:#666">.</span>rvs(n<span style="color:#666">=</span><span style="color:#666">10</span>, p<span style="color:#666">=</span><span style="color:#666">0.5</span>, size<span style="color:#666">=</span><span style="color:#666">10000</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">sns<span style="color:#666">.</span>distplot(b, hist<span style="color:#666">=</span>False)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1d99f518&gt;</pre></div>
<p><img src="/BMMB554/img/lecture14/output_8_1.png" alt="png" /></p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">binom<span style="color:#666">.</span>pmf(n<span style="color:#666">=</span><span style="color:#666">10</span>, p<span style="color:#666">=</span><span style="color:#666">0.5</span>, k<span style="color:#666">=</span><span style="color:#666">5</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">0.24609375000000025</pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">sns<span style="color:#666">.</span>distplot(b,hist<span style="color:#666">=</span>False,kde_kws<span style="color:#666">=</span><span style="color:#008000">dict</span>(cumulative<span style="color:#666">=</span>True))</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1dca0860&gt;</pre></div>
<p><img src="/BMMB554/img/lecture14/output_10_1.png" alt="png" /></p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">binom<span style="color:#666">.</span>cdf(n<span style="color:#666">=</span><span style="color:#666">10</span>, p<span style="color:#666">=</span><span style="color:#666">0.5</span>, k<span style="color:#666">=</span><span style="color:#666">5</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">0.6230468749999999</pre></div>
<h1 id="1-generative-models-for-discrete-data">1. Generative Models for Discrete Data</h1>

<hr />

<p>In molecular biology, many situations involve counting events: how many codons use a certain spelling, how many reads of DNA match a reference, how many CG dinucleotides are observed in a DNA sequence. These counts give us discrete variables, as opposed to quantities such as mass and intensity that are measured on continuous scales.</p>

<p>If we know the rules that the mechanisms under study follow, even if the outcomes are random, we can generate the probabilities of any events we are interested in by computations and standard probability laws. This is a top-down approach based on deduction and our knowledge of how to manipulate probabilities. In Chapter 2, you will see how to combine this with data-driven (bottom-up) statistical modeling.</p>

<h2 id="1-1-goals-for-this-chapter">1.1. Goals for this chapter</h2>

<hr />

<p>In this chapter we will:</p>

<ul>
<li>Learn how to obtain the probabilities of all possible outcomes from a given model and see how we can compare the theoretical frequencies with those observed in real data.</li>
<li>Explore a complete example of how to use the Poisson distribution to analyse data on epitope detection.</li>
<li>See how we can experiment with the most useful generative models for discrete data: Poisson, binomial, multinomial.</li>
<li>Use the Python for computing probabilities and counting rare events.</li>
<li>Generate random numbers from specified distributions.</li>
</ul>

<h2 id="1-2-a-real-example">1.2. A real example</h2>

<hr />

<p>Let’s dive into a real example, where we know the probability model for the process. We are told that mutations along the genome of HIV (Human Immunodeficiency Virus) occur at random with a rate of $5 \times 10^{-4}$ per nucleotide per replication cycle (generation). This means that after one cycle, the number of mutations in a genome of about $10^{4} = 10,000$ nucleotides will follow a <a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson distribution</a> (we will give more details later about this type of probability distribution) with rate 5. What does that tell us? This probability model predicts that the number of mutations over one replication cycle will be close to 5, and that the variability of this estimate is $\sqrt{5}$
(the standard error). We now have baseline reference values for both the number of mutations we expect to see in a typical HIV strain and its variability.</p>

<p>In fact, we can deduce even more detailed information. If we want to know how often 3 mutations could occur under the Poisson(5) model, we can use a Python function to generate the probability of seeing $x = 3$ events, taking the value of the rate parameter of the Poisson distribution, called lambda ($\lambda$). (Greek letters such as $\lambda$ and $\mu$ often denote important parameters that characterize the probability distributions we use.), to be 5:
.</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#008000;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">scipy.stats</span> <span style="color:#008000;font-weight:bold">import</span> poisson</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#408080;font-style:italic"># Here k = 3 and lambda = 5</span>

poisson<span style="color:#666">.</span>pmf(<span style="color:#666">3</span>,<span style="color:#666">5</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">0.1403738958142805</pre></div>
<p>This says the chance of seeing exactly three events is around 0.14, or about 1 in 7.</p>

<p>If we want to generate the probabilities of all values from 0 to 12, we do not need to write a loop. We can simply set the first argument to be the vector of these 13 values, using <code>numpy</code>&rsquo;s <code>arange</code> function. We can see the probabilities by plotting them (Figure 1.1). As with this figure, most figures in the margins of this book are created by the code shown in the text.</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">sns<span style="color:#666">.</span>barplot(x<span style="color:#666">=</span>np<span style="color:#666">.</span>arange(<span style="color:#666">13</span>), y<span style="color:#666">=</span>poisson<span style="color:#666">.</span>pmf(np<span style="color:#666">.</span>arange(<span style="color:#666">13</span>),<span style="color:#666">5</span>), color<span style="color:#666">=</span><span style="color:#ba2121">&#39;r&#39;</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1dd630f0&gt;</pre></div>
<p><img src="/BMMB554/img/lecture14/output_18_1.png" alt="png" /></p>

<blockquote>
<p><strong>Figure 1.1</strong>: Probabilities of seeing 0,1,2,…,12 mutations, as modeled by the Poisson(5) distribution. The plot shows that we will often see 4 or 5 mutations but rarely as many as 12. The distribution continues to higher numbers (13,&hellip;.), but the probabilities will be successively smaller, and here we don’t visualize them.</p>
</blockquote>

<p>Mathematical theory tells us that the Poisson probability of seeing the value $x$ is given by the formula $\frac{e^{- \lambda} \lambda^{x}}{x!}$ Here, we’ll discuss theory from time to time, but give preference to displaying concrete numeric examples and visualizations like Figure 1.1.</p>

<p>The Poisson distribution is a good model for rare events such as mutations. Other useful probability models for discrete events are the Bernoulli, binomial and multinomial distributions. We will explore these models in this chapter.</p>

<h2 id="1-3-using-discrete-probability-models">1.3. Using discrete probability models</h2>

<hr />

<p>A point mutation can either occur or not; it is a binary event. The two possible outcomes (yes, no) are called the levels of the categorical variable.</p>

<p>Think of a categorical variable as having different alternative values. These are the levels, similar to the different alternatives at a gene locus: alleles.</p>

<p>Not all events are binary. For example, the genotypes in a diploid organism can take three levels (<code>AA</code>, <code>Aa</code>, <code>aa</code>).</p>

<p>Sometimes the number of levels in a categorical variable is very large; examples include the number of different types of bacteria in a biological sample (hundreds or thousands) and the number of codons formed of 3 nucleotides (64 levels).</p>

<p>When we measure a categorical variable on a sample, we often want to tally the frequencies of the different levels in a vector of counts. <code>Pandas</code> has a special factorizing function. Here we capture the different blood genotypes for 19 subjects in a vector which we tabulate:</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">genotype <span style="color:#666">=</span> pd<span style="color:#666">.</span>Series([<span style="color:#ba2121">&#34;AA&#34;</span>,<span style="color:#ba2121">&#34;AO&#34;</span>,<span style="color:#ba2121">&#34;BB&#34;</span>,<span style="color:#ba2121">&#34;AO&#34;</span>,<span style="color:#ba2121">&#34;OO&#34;</span>,<span style="color:#ba2121">&#34;AO&#34;</span>,<span style="color:#ba2121">&#34;AA&#34;</span>,<span style="color:#ba2121">&#34;BO&#34;</span>,<span style="color:#ba2121">&#34;BO&#34;</span>,<span style="color:#ba2121">&#34;AO&#34;</span>,<span style="color:#ba2121">&#34;BB&#34;</span>,<span style="color:#ba2121">&#34;AO&#34;</span>,<span style="color:#ba2121">&#34;BO&#34;</span>,<span style="color:#ba2121">&#34;AB&#34;</span>,<span style="color:#ba2121">&#34;OO&#34;</span>,<span style="color:#ba2121">&#34;AB&#34;</span>,<span style="color:#ba2121">&#34;BB&#34;</span>,<span style="color:#ba2121">&#34;AO&#34;</span>,<span style="color:#ba2121">&#34;AO&#34;</span>])</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">genotypeF <span style="color:#666">=</span> genotype<span style="color:#666">.</span>value_counts()</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">genotypeF</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">AO    7
BB    3
BO    3
AA    2
AB    2
OO    2
dtype: int64</pre></div>
<p>If the order in which the data are observed doesn’t matter, we call the random variable exchangeable. In that case, all the information available in the factor is summarized by the counts of the factor levels. We then say that the vector of frequencies is sufficient to capture all the relevant information in the data, thus providing an effective way of compressing the data.</p>

<h3 id="1-3-1-bernoulli-trials">1.3.1. Bernoulli trials</h3>

<p>Tossing a coin has two possible outcomes. This simple experiment, called a Bernoulli trial, is modeled using a so-called Bernoulli random variable. Understanding this building block will take you surprisingly far. We can use it to build more complex models.</p>

<p>Let’s try a few experiments to see what some of these random variables look like. We use Python <code>bernoulli</code>, function for to generate outcomes for each type of distribution.</p>

<p>Suppose we want to simulate a sequence of 15 fair coin tosses. To get the outcome of 15 Bernoulli trials with a probability of success equal to 0.5 (a fair coin), we write:</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#008000;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">scipy.stats</span> <span style="color:#008000;font-weight:bold">import</span> bernoulli
bernoulli<span style="color:#666">.</span>rvs(size<span style="color:#666">=</span><span style="color:#666">15</span>,p<span style="color:#666">=</span><span style="color:#666">0.5</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">array([0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1])</pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">display(Image(<span style="color:#ba2121">&#39;./img/BallsinBoxes2.jpg&#39;</span>,width<span style="color:#666">=</span><span style="color:#666">300</span>, height<span style="color:#666">=</span><span style="color:#666">300</span>))</code></pre></div>
<p><img src="/BMMB554/img/lecture14/output_28_0.jpeg" alt="jpeg" /></p>

<blockquote>
<p><strong>Figure 1.2</strong>: Two possible events with unequal probabilities. We model this by a Bernoulli distribution with probability parameter $p=\frac{2}{3}$.</p>
</blockquote>

<p>We use the <code>bernoulli</code> function with a specific set of parameters. The first parameter, <code>size</code>, is the number of trials we want to observe; here we chose 15. We designate by <code>p</code> the probability of success.</p>

<p>Success and failure can have unequal probabilities in a Bernoulli trial, as long as the probabilities sum to 1 (We call such events complementary.). To simulate twelve trials of throwing a ball into the two boxes as shown in Figure 1.2, with probability of falling in the right-hand box $\frac{2}{3}$ and in the left-hand box $\frac{1}{3}$, we write:</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">bernoulli<span style="color:#666">.</span>rvs(size<span style="color:#666">=</span><span style="color:#666">15</span>,p<span style="color:#666">=</span><span style="color:#666">2</span><span style="color:#666">/</span><span style="color:#666">3</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">array([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0])</pre></div>
<p>The 1 indicates success, meaning that the ball fell in the right-hand box, 0 means the ball fell in the left-hand box.</p>

<h3 id="1-3-2-binomial-success-counts">1.3.2. Binomial success counts</h3>

<p>If we only care how many balls go in the right-hand box, then the order of the throws doesn’t matter (the exchangeability property), and we can get this number by just taking the sum of the cells in the output vector. Therefore, instead of the binary vector we saw above, we only need to report a single number. In Python, we can do this using one call to the binom function with the parameter <code>size</code> set to 1 and <code>n</code> set to 12:</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#008000;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">scipy.stats</span> <span style="color:#008000;font-weight:bold">import</span> binom
binom<span style="color:#666">.</span>rvs(n<span style="color:#666">=</span><span style="color:#666">12</span>, size<span style="color:#666">=</span><span style="color:#666">1</span>, p<span style="color:#666">=</span><span style="color:#666">2</span><span style="color:#666">/</span><span style="color:#666">3</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">array([5])</pre></div>
<p>This output tells us how many of the twelve balls fell into the right-hand box (the outcome that has probability <sup>2</sup>&frasl;<sub>3</sub>). We use a random two-box model when we have only two possible outcomes such as heads or tails, success or failure, CpG or non-CpG, M or F, Y = pyrimidine or R = purine, diseased or healthy, true or false. We only need the probability of “success” $p$, because “failure” (the complementary event) will occur with probability $1-p$ . When looking at the result of several such trials, if they are exchangeable (one situation in which trials are exchangeable is if they are independent of each other.), we record only the number of successes. Therefore, $SSSSSFSSSSFFFSF$ is summarized as ( #Successes=10, #Failures=5 ), or as $x = 10, n = 15$.</p>

<p>The number of successes in 15 Bernoulli trials with a probability of success of 0.3 is called a binomial random variable or a random variable that follows the $B(15,0.3)$ distribution. To generate samples, we use a call to the <code>binom</code> function with the number of trials set to 15:</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">binom<span style="color:#666">.</span>rvs(n<span style="color:#666">=</span><span style="color:#666">15</span>, size<span style="color:#666">=</span><span style="color:#666">1</span>, p<span style="color:#666">=.</span><span style="color:#666">3</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">array([3])</pre></div>
<p>The complete probability mass distribution is available by typing:</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">binom<span style="color:#666">.</span>pmf(k<span style="color:#666">=</span>np<span style="color:#666">.</span>arange(<span style="color:#666">15</span>),n<span style="color:#666">=</span><span style="color:#666">15</span>, p<span style="color:#666">=</span><span style="color:#666">0.3</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">array([4.74756151e-03, 3.05200383e-02, 9.15601148e-02, 1.70040213e-01,
       2.18623131e-01, 2.06130381e-01, 1.47235986e-01, 8.11300333e-02,
       3.47700143e-02, 1.15900048e-02, 2.98028694e-03, 5.80575378e-04,
       8.29393397e-05, 8.20279184e-06, 5.02211745e-07])</pre></div>
<p>or in rounded form:</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">np<span style="color:#666">.</span>around(binom<span style="color:#666">.</span>pmf(k<span style="color:#666">=</span>np<span style="color:#666">.</span>arange(<span style="color:#666">15</span>),n<span style="color:#666">=</span><span style="color:#666">15</span>, p<span style="color:#666">=</span><span style="color:#666">0.3</span>),decimals<span style="color:#666">=</span><span style="color:#666">3</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">array([0.005, 0.031, 0.092, 0.17 , 0.219, 0.206, 0.147, 0.081, 0.035,
       0.012, 0.003, 0.001, 0.   , 0.   , 0.   ])</pre></div>
<p>We can produce a bar plot of this distribution:</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">sns<span style="color:#666">.</span>barplot(x<span style="color:#666">=</span>np<span style="color:#666">.</span>arange(<span style="color:#666">15</span>), y<span style="color:#666">=</span>binom<span style="color:#666">.</span>pmf(k<span style="color:#666">=</span>np<span style="color:#666">.</span>arange(<span style="color:#666">15</span>),n<span style="color:#666">=</span><span style="color:#666">15</span>, p<span style="color:#666">=</span><span style="color:#666">0.3</span>), color<span style="color:#666">=</span><span style="color:#ba2121">&#39;r&#39;</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1de89f98&gt;</pre></div>
<p><img src="/BMMB554/img/lecture14/output_42_1.png" alt="png" /></p>

<blockquote>
<p><strong>Figure 1.3.</strong> Theoretical distribution of $B(15,0.3)$. The highest bar is at $x=4$. We have chosen to represent theoretical values in red throughout.</p>
</blockquote>

<p>The number of trials is the number we input in Python as $n$, while the probability of success is $p$. Mathematical theory tells us that for $X$ distributed as a binomial distribution with parameters $(n, p)$ written $X \sim B(n,p)$, the probability of seeing $X = k$ successes is:</p>

<p>$P(X = k) = \binom{n}{k}p^k(1-p)^{n-k}$</p>

<p>where bionomial notation $\binom{n}{k} = \frac{n!}{(n-k)!k!}$</p>

<p>&#9757; What is the output of the formula for $k=3$, $p=<sup>2</sup>&frasl;<sub>3</sub>$, $n=4$?</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">binom<span style="color:#666">.</span>pmf(k<span style="color:#666">=</span><span style="color:#666">3</span>,p<span style="color:#666">=</span><span style="color:#666">2</span><span style="color:#666">/</span><span style="color:#666">3</span>,n<span style="color:#666">=</span><span style="color:#666">4</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">0.39506172839506176</pre></div>
<h3 id="1-3-3-poisson-distributions">1.3.3. Poisson distributions</h3>

<p>When the probability of success $p$ is small and the number of trials $n$ large, the binomial distribution $B(n,p)$ can be faithfully approximated by a simpler distribution, the Poisson distribution with rate parameter $λ=np$. We already used this fact, and this distribution, in the HIV example above</p>

<p>&#9757; What is the probability mass distribution of observing 0:12 mutations in a genome of $n=10^4$ nucleotides, when the probability is $p=5\times10^{−4}$ per nuceotide? Is it similar when modeled by the binomial $B(n,p)$ distribution and by the Poisson $(λ=np)$ distribution?</p>

<p>Note that, unlike the binomial distribution, the Poisson no longer depends on two separate parameters $n$ and $p$, but only on their product $np$. As in the case of the binomial distribution, we also have a mathematical formula for computing Poisson probabilities:</p>

<p>$P(X = k) = \frac{λ^ke^{-λ}}{k!}$</p>

<p>thus for our example above $λ = (5\times10^{-4})\times(10^4) = 5$ and the probability of observing, say, 3 mutations will be:</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#008000;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">math</span>
<span style="color:#666">5</span><span style="color:#666">**</span><span style="color:#666">3</span> <span style="color:#666">*</span> math<span style="color:#666">.</span>exp(<span style="color:#666">-</span><span style="color:#666">5</span>) <span style="color:#666">/</span> math<span style="color:#666">.</span>factorial(<span style="color:#666">3</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">0.14037389581428056</pre></div>
<p>or (in SciPy <code>mu</code> is used in place of λ):</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">poisson<span style="color:#666">.</span>pmf(k<span style="color:#666">=</span><span style="color:#666">3</span>, mu<span style="color:#666">=</span><span style="color:#666">5</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">0.1403738958142805</pre></div>
<p>Let&rsquo;s now compare results of simulating data using bionomial and poisson distribution along 10,000 positions (this is our genome size) with a mutation rate of $5\times10^{-4}$. For binomial distribution is will be:</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">sns<span style="color:#666">.</span>distplot(binom<span style="color:#666">.</span>rvs(n<span style="color:#666">=</span><span style="color:#666">10000</span>, p<span style="color:#666">=</span><span style="color:#666">5e-4</span>, size<span style="color:#666">=</span><span style="color:#666">300000</span>))</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1ded9128&gt;</pre></div>
<p><img src="/BMMB554/img/lecture14/output_54_1.png" alt="png" /></p>

<p>and for Poisson:</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">sns<span style="color:#666">.</span>distplot(poisson<span style="color:#666">.</span>rvs(mu<span style="color:#666">=</span><span style="color:#666">5</span>,size<span style="color:#666">=</span><span style="color:#666">300000</span>))</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1e36b9e8&gt;</pre></div>
<p><img src="/BMMB554/img/lecture14/output_56_1.png" alt="png" /></p>

<h3 id="1-3-4-a-generative-model-for-epitope-detection">1.3.4. A generative model for epitope detection</h3>

<p>When testing certain pharmaceutical compounds, it is important to detect proteins that provoke an allergic reaction. The molecular sites that are responsible for such reactions are called epitopes. The technical definition of an epitope is:</p>

<blockquote>
<p>A specific portion of a macromolecular antigen to which an antibody binds. In the case of a protein antigen recognized by a T-cell, the epitope or determinant is the peptide portion or site that binds to a Major Histocompatibility Complex (MHC) molecule for recognition by the T cell receptor (TCR).</p>
</blockquote>

<p>And in case you’re not so familar with immunology: an antibody is a type of protein made by certain white blood cells in response to a foreign substance in the body, which is called the antigen.</p>

<p>An antibody binds (with more or less specificity) to its antigen. The purpose of the binding is to help destroy the antigen. Antibodies can work in several ways, depending on the nature of the antigen. Some antibodies destroy antigens directly. Others help recruit white blood cells to destroy the antigen. An epitope, also known as antigenic determinant, is the part of an antigen that is recognized by the immune system, specifically by antibodies, B cells or T cells.</p>

<h4 id="elisa-error-model-with-known-parameters">ELISA error model with known parameters</h4>

<p><a href="http://en.wikipedia.org/wiki/ELISA">ELISA</a> assays are used to detect specific epitopes at different positions along a protein. Suppose the following facts hold for an ELISA array we are using:</p>

<ul>
<li>The baseline noise level per position, or more precisely the false positive rate, is 1%. This is the probability of declaring a hit – we think we have an epitope – when there is none. We write this $P(declare\ epitope | no\ epitope)$</li>
<li>The protein is tested at 100 different positions, supposed to be independent</li>
<li>We are going to examine a collection of 50 patient samples</li>
</ul>

<h4 id="one-patient-s-data">One patient&rsquo;s data</h4>

<p>The data for one patient’s assay look like this:</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0</pre></div>
<p>where the 1 signifies a hit (and thus the potential for an allergic reaction), and the zeros signify no reaction at that position.</p>

<p>&#9757; Verify by simulation that the sum of 50 independent Bernoulli variables with $p = 0.01$ is –to good enough approximation– the same as a Poisson(0.5) random variable.</p>

<h4 id="results-from-50-assays">Results from 50 assays</h4>

<p>We’re going to study the data for all 50 patients tallied at each of the 100 positions. If there are no allergic reactions, the false positive rate means that for one patient, each individual position has a probability of 1 in 100 of being a 1. So, after tallying 50 patients, we expect at any given position the sum of the 50 observed $(0,1)$ variables to have a Poisson distribution with parameter 0.5 ($0.01\times50=0.5$). Let&rsquo;s load the data:</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="">!</span>pip install pyreadr</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">Collecting pyreadr
[?25l  Downloading https://files.pythonhosted.org/packages/eb/c1/9a3d132ba9c109adae7729f9d4371fff2c39628afbda451159dd3ead89a3/pyreadr-0.1.8-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (266kB)
[K    100% |████████████████████████████████| 276kB 5.9MB/s ta 0:00:01
[?25hInstalling collected packages: pyreadr
Successfully installed pyreadr-0.1.8</pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="">!</span>wget https:<span style="color:#666">//</span>github<span style="color:#666">.</span>com<span style="color:#666">/</span>nekrut<span style="color:#666">/</span>msmb_python<span style="color:#666">/</span>raw<span style="color:#666">/</span>master<span style="color:#666">/</span>data<span style="color:#666">/</span>e100<span style="color:#666">.</span>RData</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">--2019-03-18 13:41:42--  https://github.com/nekrut/msmb_python/raw/master/data/e100.RData
Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113
Connecting to github.com (github.com)|192.30.253.112|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://raw.githubusercontent.com/nekrut/msmb_python/master/data/e100.RData [following]
--2019-03-18 13:41:43--  https://raw.githubusercontent.com/nekrut/msmb_python/master/data/e100.RData
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.184.133
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.184.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 129 [application/octet-stream]
Saving to: ‘e100.RData’

e100.RData          100%[===================&gt;]     129  --.-KB/s    in 0s      

2019-03-18 13:41:43 (8.20 MB/s) - ‘e100.RData’ saved [129/129]</pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#008000;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">pyreadr</span>
df <span style="color:#666">=</span> pyreadr<span style="color:#666">.</span>read_r(<span style="color:#ba2121">&#39;e100.RData&#39;</span>)[<span style="color:#ba2121">&#39;e100&#39;</span>]</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">df<span style="color:#666">.</span>head()</code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table class="table table-striped table-responsive">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>e100</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>

<p>and now let&rsquo;s visuzalise it:</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">g <span style="color:#666">=</span> sns<span style="color:#666">.</span>barplot(data<span style="color:#666">=</span>df<span style="color:#666">.</span>reset_index(),x<span style="color:#666">=</span><span style="color:#ba2121">&#39;index&#39;</span>,y<span style="color:#666">=</span><span style="color:#ba2121">&#39;e100&#39;</span>)
g<span style="color:#666">.</span><span style="color:#008000">set</span>(ylim<span style="color:#666">=</span>(<span style="color:#666">0</span>,<span style="color:#666">8</span>))
ticks <span style="color:#666">=</span> np<span style="color:#666">.</span>arange(<span style="color:#666">0</span>, <span style="color:#666">101</span>, step<span style="color:#666">=</span><span style="color:#666">10</span>)
plt<span style="color:#666">.</span>xticks(ticks,ticks)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">([&lt;matplotlib.axis.XTick at 0x1a1f62d898&gt;,
  &lt;matplotlib.axis.XTick at 0x1a1f62d208&gt;,
  &lt;matplotlib.axis.XTick at 0x1a1f62d0b8&gt;,
  &lt;matplotlib.axis.XTick at 0x1a1f7fb198&gt;,
  &lt;matplotlib.axis.XTick at 0x1a1f7fb630&gt;,
  &lt;matplotlib.axis.XTick at 0x1a1f7fbb00&gt;,
  &lt;matplotlib.axis.XTick at 0x1a1f7fbd30&gt;,
  &lt;matplotlib.axis.XTick at 0x1a1f7fb780&gt;,
  &lt;matplotlib.axis.XTick at 0x1a1f804400&gt;,
  &lt;matplotlib.axis.XTick at 0x1a1f804940&gt;,
  &lt;matplotlib.axis.XTick at 0x1a1f804e10&gt;],
 &lt;a list of 11 Text xticklabel objects&gt;)</pre></div>
<p><img src="/BMMB554/img/lecture14/output_66_1.png" alt="png" /></p>

<p>The spike in the figure above is striking. What are the chances of seeing a value as large as 7, if no epitope is present?
If we look for the probability of seeing a number as big as 7 (or larger) when considering one $Poisson(0.5)$ random variable, the answer can be calculated in closed form as:</p>

<p>$P(X\geq7)=\sum_{k=7}^\infty P(X=k)$</p>

<p>which is the same as $1-P(X\leq6)$. The probability $1-P(X\leq6)$ s the so-called cumulative distribution function at 6, and R has the function ppois for computing it, which we can use in either of the following two ways (Besides the convenience of not having to do the subtraction from one, the second of these computations also tends to be more accurate when the probability is small. This has to do with limitations of floating point arithmetic):</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#666">1</span><span style="color:#666">-</span>poisson<span style="color:#666">.</span>cdf(<span style="color:#666">6</span>,<span style="color:#666">0.5</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">1.0023796028768572e-06</pre></div>
<p>above we use <code>poisson.cdf</code>, which is cumulative distribution function</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">poisson<span style="color:#666">.</span>sf(<span style="color:#666">6</span>,<span style="color:#666">0.5</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">1.0023796028842995e-06</pre></div>
<p>and here we use <code>poisson.sf</code>, which is <a href="https://en.wikipedia.org/wiki/Survival_function">survival function</a> defined as 1 - cdf.</p>

<p>We denote this number by $\epsilon$, the Greek letter epsilon (Mathematicians often call small numbers (and children)
$\epsilon$). We have shown that the probability of seeing a count as large as $7$ , assuming no epitope reactions, is:</p>

<p>$\epsilon = (P\geq7) = 1-P(X\leq6)\simeq10^{-6}$</p>

<h4 id="extreme-value-analysis-for-the-poisson-distribution">Extreme value analysis for the Poisson distribution</h4>

<p>Stop! The above calculation is not the correct computation in this case. Can you spot the flaw in our reasoning if we want to compute the probability that we observe these data if there is no epitope? We looked at all 100 positions, looked for the largest value and found that it was 7. Due to this selection, a value as large as 7 is more likely to occur than if we only looked at one position.</p>

<p>So instead of asking what the chances are of seeing a Poisson(0.5) as large as 7, we should ask ourselves, what are the chances that the maximum of 100 Poisson(0.5) trials is as large as 7? We use extreme value analysis here (Meaning that we’re interested in the behavior of the very large or very small values of a random distribution, for instance the maximum or the minimum.). We order the data values $x_1,x<em>2,&hellip;,x</em>{100}$ and rename them  $x<em>{(1)},x</em>{(2)},&hellip;,x<em>{(100)}$ so that $x</em>{(1)}$ denotes the smallest and $x<em>{(100)}$ the largest of the counts over 100 positions. Together, $x</em>{(1)},x<em>{(2)},&hellip;,x</em>{(100)}$ are called <strong>rank statistic</strong> of this sample of 100 values. The maximum value being as large as 7 is the complementary event of having all 100 counts be smaller than or equal to 6. Two complementary events have probabilities that sum to 1. Because the positions are supposed to be independent, we can now do the computation (the notation with the $\prod$  is just a compact way to write the product of a series of terms, analogous to the $\sum$ for sums):</p>

<p>$P(x<em>{(100)}\geq7)=1-\prod</em>{i=1}^{100}P(x_i\leq6)$</p>

<p>Because we suppose each of these 100 events are independent, we can use our result from above:</p>

<p>$\prod_{i=1}^{100}P(x_i\leq6)=(P(x_i\leq6))^{100}=(1-\epsilon)^{100}$</p>

<h4 id="actually-computing-the-numbers">Actually computing the numbers</h4>

<p>We could just let Python compute the value of this number, $(1−\epsilon)^{100}$. For those interested in how such calculations can be shortcut through approximation, we give some details. These can be skipped on a first reading.</p>

<p>We recall from above that $\epsilon \simeq 10^{-6}$ is <em>much</em> smaller than 1. To compute the value of $(1−\epsilon)^{100}$ approximately, we can use the <a href="https://en.wikipedia.org/wiki/Binomial_theorem">binomial theorem</a> and drop all “higher order” terms of $\epsilon$, i.e., all terms with $\epsilon^2,  \epsilon^3, &hellip;$ because they are negligibly small compared to the remaining (“leading”) terms.</p>

<p>$(1-\epsilon)^{100}=\sum\limits_{k=0}^n\binom{n}{k}1^{n-k}(-\epsilon)^k=1-n\epsilon+\binom{n}{2}\epsilon^2-\binom{n}{3}\epsilon^3+&hellip;\simeq1-n\epsilon\simeq1-10^{-4} $</p>

<p>Another, equivalent, route goes by using the approximation $e^{-\epsilon}\simeq1-\epsilon$, which is the same as $log(1-\epsilon)\simeq-\epsilon$. Hence:</p>

<p>$(1-\epsilon)^{100}=e^{log((1-\epsilon)^{100})}=e^{100log(1-\epsilon)}\simeq e^{-100\epsilon}\simeq e^{-10^{-4}}\simeq 1 - 10^{-4}$.</p>

<p>Thus the correct probability of seeing a number of hits as large or larger than 7 in the 100 positions, if there is no epitope, is about 100 times the probability we wrongly calculated previously. Both computed probabilities $10^{−6}$ and $10^{−4}$ are smaller than standard significance thresholds (say, $0.05$, $0.01$ or
$0.001$). The decision to reject the null of no epitope would have been the same. However if one has to stand up in court and defend the $p$-value to 8 significant digits as in some forensic court cases(this occurred in the examination of the forensic evidence in the OJ Simpson case.) that is another matter. The adjusted $p$-value that takes into account the multiplicity of the test is the one that should be reported, and we will return to this important issue in the next several lectures.</p>

<h4 id="computing-probabilities-by-simulation">Computing probabilities by simulation</h4>

<p>In the case we just saw, the theoretical probability calculation was quite simple and we could figure out the result by an explicit calculation. In practice, things tend to be more complicated, and we are better to compute our probabilities using the <strong><a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte Carlo</a></strong> method: a computer simulation based on our generative model that finds the probabilities of the events we’re interested in. Below, we generate 100,000 instances of picking the maximum from 100 Poisson distributed numbers.</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">maxes <span style="color:#666">=</span> np<span style="color:#666">.</span>array([<span style="color:#008000">max</span>(poisson<span style="color:#666">.</span>rvs(mu<span style="color:#666">=</span><span style="color:#666">0.5</span>,size<span style="color:#666">=</span><span style="color:#666">100</span>)) <span style="color:#008000;font-weight:bold">for</span> i <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#008000">range</span>(<span style="color:#666">100000</span>)])</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">y,x <span style="color:#666">=</span> np<span style="color:#666">.</span>histogram(maxes,bins<span style="color:#666">=</span>[<span style="color:#666">0</span>,<span style="color:#666">1</span>,<span style="color:#666">2</span>,<span style="color:#666">3</span>,<span style="color:#666">4</span>,<span style="color:#666">5</span>,<span style="color:#666">6</span>,<span style="color:#666">7</span>,<span style="color:#666">8</span>,<span style="color:#666">9</span>])</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#408080;font-style:italic"># Here we use x[:9] because bin array returned by numpy.histogram is one element longer than histogram array</span>
sns<span style="color:#666">.</span>barplot(x<span style="color:#666">=</span>x[:<span style="color:#666">9</span>], y<span style="color:#666">=</span>y, color<span style="color:#666">=</span><span style="color:#ba2121">&#39;g&#39;</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1f8f3f60&gt;</pre></div>
<p><img src="/BMMB554/img/lecture14/output_78_1.png" alt="png" /></p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">x</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">y</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">array([    0,     6, 23246, 60539, 14495,  1570,   132,    12,     0])</pre></div>
<p>In ~100 of 1,000,000 trials, the maximum was 7 or larger. This gives the following approximation for $P(X_{max}\geq7)$:</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">np<span style="color:#666">.</span>shape(maxes[maxes<span style="color:#666">&gt;=</span><span style="color:#666">7</span>])[<span style="color:#666">0</span>]<span style="color:#666">/</span><span style="color:#666">1000000</span></code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">1.2e-05</pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#666">12</span><span style="color:#666">/</span><span style="color:#666">100000</span></code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">0.00012</pre></div>
<p>which more or less agrees with our theoretical calculation. We already see one of the potential limitations of Monte Carlo simulations: the “granularity” of the simulation result is determined by the inverse of the number of simulations (100000) and so will be around $10^{-5}$. Any estimated probability cannot be more precise than this granularity, and indeed the precision of our estimate will be a few multiples of that. Everything we have done up to now is only possible because we know the false positive rate per position, we know the number of patients assayed and the length of the protein, we suppose we have identically distributed independent draws from the model, and there are no unknown parameters (we postulated the Poisson distribution for the noise, pretending we knew all the parameters and were able to conclude through mathematical deduction). This is an example of <strong>probability</strong> or <strong>generative modeling</strong>: all the parameters are known and the mathematical theory allows us to work by deduction in a top-down fashion.</p>

<p>If instead we are in the more realistic situation of knowing the number of patients and the length of the proteins, but don’t know the distribution of the data, then we have to use statistical modeling. This approach will be developed in the next chapter. We will see that if we have only the data to start with, we first need to <strong>fit</strong> a reasonable distribution to describe it. However, before we get to this harder problem, let’s extend our knowledge of discrete distributions to more than binary, success-or-failure outcomes.</p>

<h2 id="1-4-multinomial-distributions-the-case-for-dna">1.4. Multinomial distributions: the case for DNA</h2>

<h4 id="more-than-two-outcomes">More than two outcomes</h4>

<p>When modeling four possible outcomes, as for instance the boxes in Figure 1.9 or when studying counts of the four nucleotides A,C,G and T, we need to extend the binomial model.</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">display(Image(<span style="color:#ba2121">&#39;./img/BallsinBoxes4.jpg&#39;</span>,width<span style="color:#666">=</span><span style="color:#666">300</span>, height<span style="color:#666">=</span><span style="color:#666">300</span>))</code></pre></div>
<p><img src="/BMMB554/img/lecture14/output_87_0.jpeg" alt="jpeg" /></p>

<blockquote>
<p><strong>Figure 1.9</strong>: The boxes represent four outcomes or levels of a discrete categorical variable. The box on the right represents the more likely outcome.</p>
</blockquote>

<p>Recall that when using the binomial, we can consider unequal probabilities for the two outcomes by assigning a probability $p=P(1)=p_1$ to th eoutcome 1 and $1-p=p(0)=p_0$ to th eoutcome 0. When there are more than two possible outcomes, say A,C,G and T, we can think of throwing balls into boxes of differing sizes corresponding to different probabilities, and we can label these probabilites $p_A$, $p_C$, $p_G$, $p_T$. Just as in the binomial case the sum of the probabilities of all possible outcomes is $p_A + p_C + p_G + p_T = 1$.</p>

<p>Experiment with the random number generator that generates all possible numbers between 0 and 1 through the function called <code>uniform.rvs</code>. Use it to generate a random variable with 4 levels (A, C, G, T) where $p_A=\frac{1}{8}$,$p_C=\frac{3}{8}$, $p_G=\frac{3}{8}$, $p_T=\frac{1}{8}$. To do this build on the following expression:</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#008000;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">scipy.stats</span> <span style="color:#008000;font-weight:bold">import</span> uniform
uniform<span style="color:#666">.</span>rvs(scale<span style="color:#666">=</span><span style="color:#666">8</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">3.206321601915464</pre></div>
<h4 id="mathematics-formulation">Mathematics formulation</h4>

<p>Multinomial distributions are the most important model for tallying counts and Python uses a general formula to compute the probability of a multinomial vector of counts $(x_1,&hellip;,x_m)$ for outcomes that have $m$ boxes with probabilities $p_1,&hellip;,p_m$:</p>

<p>$P(x_1,x_2,&hellip;,x_m | p_1, p_2, &hellip; ,p_m) = \binom{n}{x_1,x_2,&hellip;,x_m}p_1^{x_1}p_2^{x_2}&hellip;p_m^{x_m}$</p>

<p>&#9757; Suppose we have four boxes that are equally likely. Using the formula, what is the probability of observing 4 in the first box, 2 in the second box, and none in the two other boxes?</p>

<p>$P(4,2,0,0) = \frac{6!}{4!2!}\times(\frac{1}{4})^6=\frac{15}{4^6}\simeq0.0037$</p>

<p>or in Python:</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#008000;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">scipy.stats</span> <span style="color:#008000;font-weight:bold">import</span> multinomial
multinomial<span style="color:#666">.</span>pmf(x<span style="color:#666">=</span>(<span style="color:#666">4</span>,<span style="color:#666">2</span>,<span style="color:#666">0</span>,<span style="color:#666">0</span>),n<span style="color:#666">=</span><span style="color:#666">6</span>,p<span style="color:#666">=</span>[<span style="color:#666">0.25</span>]<span style="color:#666">*</span><span style="color:#666">4</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">0.003662109374999998</pre></div>
<p>We often run simulation experiments to check whether the data we see are consistent with the simplest possible four-box model where each box has the same probability <sup>1</sup>&frasl;<sub>4</sub>. In some sense it is the strawman (nothing interesting is happening). We’ll see more examples of this in the next lecture. Here we use a few R commands to generate such vectors of counts. First suppose we have 8 characters of four different, equally likely types:</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">pvec<span style="color:#666">=</span>[<span style="color:#666">1</span><span style="color:#666">/</span><span style="color:#666">4</span>]<span style="color:#666">*</span><span style="color:#666">4</span>
multinomial<span style="color:#666">.</span>rvs(n<span style="color:#666">=</span><span style="color:#666">8</span>,p<span style="color:#666">=</span>pvec,size<span style="color:#666">=</span><span style="color:#666">1</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">array([[2, 1, 2, 3]])</pre></div>
<p>&#9757;  How do you interpret the difference between <code>multinomial.rvs(n=8,p=pvec,size=1)</code> and <code>multinomial.rvs(n=1,p=pvec,size=8)</code>?</p>

<h3 id="1-4-1-simulating-for-power">1.4.1. Simulating for power</h3>

<p>Let’s see an example of using Monte Carlo for the multinomial in a way which is related to a problem scientists often have to solve when planning their experiments: how big a sample size do I need?</p>

<p>The term power has a special meaning in statistics. It is the probability of detecting something if it is there, also called the true positive rate.</p>

<p>Conventionally, experimentalists aim for a power of 80% (or more) when planning experiments. This means that if the same experiment is run many times, about 20% of the time it will fail to yield significant results even though it should.</p>

<p>Let’s call $H_0$  the null hypothesis that the DNA data we have collected comes from a fair process, where each of the 4 nucleotides is equally likely $(p_A,p_C,p_G,p_T)=(0.25,0.25,0.25,0.25)$</p>

<p>Null here just means: the baseline, where nothing interesting is going on. It’s the strawman that we are trying to disprove (or “reject”, in the lingo of statisticians), so the null hypothesis should be such that deviations from it are interesting</p>

<blockquote>
<p>If you know a little biology, you will know that DNA of living organisms rarely follows that null hypothesis – so disproving it may not be all that interesting. Here we proceed with this null hypothesis because it allows us to illustrate the calculations, but it can also serve us as a reminder that the choice of a good null hypothesis (one whose rejection is interesting) requires scientific input)</p>
</blockquote>

<p>As you saw by running the Python commands for 8 characters and 4 equally likely outcomes, represented by equal-sized boxes, we do not always get 2 in each box. It is impossible to say, from looking at just 8 characters, whether the nucleotides come from a fair process or not.</p>

<p>Let’s determine if, by looking at a sequence of length $n=20$, we can detect whether the original distribution of nucleotides is fair or whether it comes from some other (“alternative”) process.</p>

<p>We generate 1000 simulations from the null hypothesis using the rmultinom function. We display only the first 11 columns to save space.</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">obsunder0 <span style="color:#666">=</span> np<span style="color:#666">.</span>transpose(multinomial<span style="color:#666">.</span>rvs(n<span style="color:#666">=</span><span style="color:#666">20</span>,p<span style="color:#666">=</span>pvec,size<span style="color:#666">=</span><span style="color:#666">1000</span>))</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">obsunder0<span style="color:#666">.</span>shape</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">(4, 1000)</pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">obsunder0[:,:<span style="color:#666">11</span>]</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">array([[3, 4, 6, 5, 3, 7, 7, 6, 7, 7, 6],
       [7, 6, 4, 4, 5, 1, 8, 6, 4, 2, 4],
       [4, 6, 5, 7, 6, 5, 3, 3, 3, 5, 5],
       [6, 4, 5, 4, 6, 7, 2, 5, 6, 6, 5]])</pre></div>
<p>Each column in the matrix <code>obsunder0</code> is a simulated instance. You can see that the numbers in the boxes vary a lot: some are as big as 10, whereas the expected value is $5=<sup>20</sup>&frasl;<sub>4</sub>$.</p>

<h4 id="creating-a-test">Creating a test</h4>

<p>Remember: we know these values come from a fair process. Clearly, knowing the process’ expected values isn’t enough. We also need a measure of variability that will enable us to describe how much variability is expected and how much is too much. We use as our measure the following statistic, which is computed as the sum of the squares of the differences between the observed values and the expected values relative to the expected values (This measure weights each of the square residuals relative to their expected values.). Thus, for each instance,</p>

<p>$stat=\frac{(E_A-x_A)^2}{E_A}+\frac{(E_C-x_C)^2}{E_C}+\frac{(E_G-x_G)^2}{E_G}+\frac{(E_T-x_T)^2}{E_T}=\sum_i\frac{(E_i-x_i)^2}{E_i}\ \ \  (1.1)$</p>

<p>How much do the first three columns of the generated data differ from what we expect? We get:</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">pvec<span style="color:#666">=</span>np<span style="color:#666">.</span>array(pvec)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">expected0 <span style="color:#666">=</span> np<span style="color:#666">.</span>array(pvec<span style="color:#666">*</span><span style="color:#666">20</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">expected0</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">array([5., 5., 5., 5.])</pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">expected0<span style="color:#666">.</span>shape</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">(4,)</pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">obsunder0<span style="color:#666">.</span>shape</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">(4, 1000)</pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">np<span style="color:#666">.</span><span style="color:#008000">sum</span>((obsunder0[:,<span style="color:#666">0</span>] <span style="color:#666">-</span> expected0)<span style="color:#666">**</span><span style="color:#666">2</span> <span style="color:#666">/</span> expected0)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">2.0</pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">np<span style="color:#666">.</span><span style="color:#008000">sum</span>((obsunder0[:,<span style="color:#666">1</span>] <span style="color:#666">-</span> expected0)<span style="color:#666">**</span><span style="color:#666">2</span> <span style="color:#666">/</span> expected0)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">0.8</pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">np<span style="color:#666">.</span><span style="color:#008000">sum</span>((obsunder0[:,<span style="color:#666">2</span>] <span style="color:#666">-</span> expected0)<span style="color:#666">**</span><span style="color:#666">2</span> <span style="color:#666">/</span> expected0)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">0.4</pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">np<span style="color:#666">.</span><span style="color:#008000">sum</span>((obsunder0[:,<span style="color:#666">3</span>] <span style="color:#666">-</span> expected0)<span style="color:#666">**</span><span style="color:#666">2</span> <span style="color:#666">/</span> expected0)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">1.2</pre></div>
<p>The values of the measure can differ: you can look at a few more than 3 columns, we’re going to see how to study all 1,000 of them. To avoid repetitive typing, we encapsulate the formula for stat, Equation (1.1), in a loop. To this we will first initialize a vector <code>S0</code> that will hold the values for our statistics fror all 1000 instances:</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">S0 <span style="color:#666">=</span> np<span style="color:#666">.</span>zeros(<span style="color:#666">1000</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#008000;font-weight:bold">for</span> i <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#008000">range</span>(obsunder0<span style="color:#666">.</span>shape[<span style="color:#666">1</span>]):
  S0[i] <span style="color:#666">=</span> np<span style="color:#666">.</span><span style="color:#008000">sum</span>((obsunder0[:,i] <span style="color:#666">-</span> expected0)<span style="color:#666">**</span><span style="color:#666">2</span> <span style="color:#666">/</span> expected0)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">sns<span style="color:#666">.</span>distplot(S0, rug<span style="color:#666">=</span>True)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1eab4e48&gt;</pre></div>
<p><img src="/BMMB554/img/lecture14/output_117_1.png" alt="png" /></p>

<blockquote>
<p>The histogram of simulated values S0 of the statistic stat under the null (fair) distribution provides an approximation of the sampling distribution of the statistic stat.</p>
</blockquote>

<p>The summary function shows us that <code>S0</code> takes on a spread of different values. From the simulated data we can approximate, for instance, the 95% quantile (a value that separates the smaller 95% of the values from the 5% largest values).</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#008000;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">scipy.stats</span> <span style="color:#008000;font-weight:bold">import</span> describe
describe(S0)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">DescribeResult(nobs=1000, minmax=(0.0, 16.8), mean=2.9276, variance=5.588506746746747, skewness=1.597350716203356, kurtosis=3.73165460972319)</pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">np<span style="color:#666">.</span>quantile(S0, <span style="color:#666">0.95</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">7.6</pre></div>
<p>So we see that 5% of the <code>S0</code> values are larger than 7.6. We’ll propose this as our critical value for testing data and will reject the hypothesis that the data come from a fair process, with equally likely nucleotides, if the weighted sum of squares stat is larger than 7.6.</p>

<h4 id="determining-test-power">Determining test power</h4>

<p>We must compute the probability that our test –based on the weighted sum-of-square differences– will detect that the data in fact do not come from the null hypothesis. We compute the probability of rejecting by simulation. We generate 1000 simulated instances from an alternative process, parameterized by <code>pvecA</code>:</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">pvecA<span style="color:#666">=</span>np<span style="color:#666">.</span>array([<span style="color:#666">3</span><span style="color:#666">/</span><span style="color:#666">8</span>,<span style="color:#666">1</span><span style="color:#666">/</span><span style="color:#666">4</span>,<span style="color:#666">3</span><span style="color:#666">/</span><span style="color:#666">12</span>,<span style="color:#666">1</span><span style="color:#666">/</span><span style="color:#666">8</span>])</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">pvecA</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">array([0.375, 0.25 , 0.25 , 0.125])</pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">observed <span style="color:#666">=</span> np<span style="color:#666">.</span>transpose(multinomial<span style="color:#666">.</span>rvs(n<span style="color:#666">=</span><span style="color:#666">20</span>,p<span style="color:#666">=</span>pvecA,size<span style="color:#666">=</span><span style="color:#666">1000</span>))</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">observed<span style="color:#666">.</span>shape</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">(4, 1000)</pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">observed[:,:<span style="color:#666">7</span>]</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">array([[9, 8, 6, 6, 9, 7, 8],
       [2, 3, 9, 5, 3, 4, 5],
       [6, 9, 2, 5, 7, 8, 4],
       [3, 0, 3, 4, 1, 1, 3]])</pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">np<span style="color:#666">.</span>mean(observed,axis<span style="color:#666">=</span><span style="color:#666">1</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">array([7.517, 4.914, 4.994, 2.575])</pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">expectedA <span style="color:#666">=</span> pvecA<span style="color:#666">*</span><span style="color:#666">20</span></code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">expectedA</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">array([7.5, 5. , 5. , 2.5])</pre></div>
<p>As with the simulation from the null hypothesis, the observed values vary considerably. The question is: how often (out of 1000 instances) will our test detect that the data depart from the null?</p>

<p>The test doesn’t reject the first observation: because the value of the statistic is within the 95th percentile.</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">observed[:,<span style="color:#666">0</span>]</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">array([9, 2, 6, 3])</pre></div>
<p>because the value of the statistic is within the 95th percentile:</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">np<span style="color:#666">.</span><span style="color:#008000">sum</span>((observed[:,<span style="color:#666">0</span>] <span style="color:#666">-</span> expectedA)<span style="color:#666">**</span><span style="color:#666">2</span> <span style="color:#666">/</span> expectedA)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">2.4000000000000004</pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">S1 <span style="color:#666">=</span> np<span style="color:#666">.</span>zeros(<span style="color:#666">1000</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#008000;font-weight:bold">for</span> i <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#008000">range</span>(observed<span style="color:#666">.</span>shape[<span style="color:#666">1</span>]):
  S1[i] <span style="color:#666">=</span> np<span style="color:#666">.</span><span style="color:#008000">sum</span>((observed[:,i] <span style="color:#666">-</span> expectedA)<span style="color:#666">**</span><span style="color:#666">2</span> <span style="color:#666">/</span> expectedA)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">S1[<span style="color:#666">1</span>:<span style="color:#666">10</span>]</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">array([6.53333333, 5.4       , 1.2       , 2.8       , 2.93333333,
       0.33333333, 2.8       , 1.93333333, 2.73333333])</pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">sns<span style="color:#666">.</span>distplot(S1)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1fa17f98&gt;</pre></div>
<p><img src="/BMMB554/img/lecture14/output_139_1.png" alt="png" /></p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">q95 <span style="color:#666">=</span> np<span style="color:#666">.</span>quantile(S1, <span style="color:#666">0.95</span>)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">q95</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">7.199999999999999</pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">np<span style="color:#666">.</span><span style="color:#008000">sum</span>(S1 <span style="color:#666">&gt;</span> q95)</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">50</pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">power <span style="color:#666">=</span> np<span style="color:#666">.</span>mean(S1 <span style="color:#666">&gt;</span> q95)
power</code></pre></div><div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4">0.05</pre></div>
<p>Run across 1000 simulations, the test identified 199 as coming from an alternative distribution. We’ve thus computed that the probability $P(reject\ H_0 | H_A)$ is 0.048.</p>

<p>We read the vertical line as given or conditional on.</p>

<p>With a sequence length of $n=20$ we have a power of about 5% to detect the difference between the fair generating process and our alternative.</p>

<p>In practice, as we mentioned, an acceptable value of power is  0.8 or more. Repeat the simulation experiments and suggest a new sequence length
$n$ that will ensure that the power is acceptable.</p>

<h4 id="classical-statistics-for-classical-data">Classical statistics for classical data</h4>

<p>We didn’t need to simulate the data using Monte Carlo to compute the 95th percentiles; there is an adequate theory to help us with the computations.</p>

<p>Our statistic stat actually has a well-known distribution called the chi-square distribution (with 3 degrees of freedom) and written $\chi_3^2$3.</p>

<p>We will see in the next lecture how to compare distributions using Q-Q plots. We could have used a more standard test instead of running a hand-made simulation. However, the procedure we’ve learned extends to many situations in which the chi-square distribution doesn’t apply. For instance, when some of the boxes have extremely low probabilities and their counts are mostly zero.</p>
<div class="highlight"><pre style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">display(Image(<span style="color:#ba2121">&#39;./img/ProbaDiagram.png&#39;</span>,width<span style="color:#666">=</span><span style="color:#666">300</span>, height<span style="color:#666">=</span><span style="color:#666">300</span>))</code></pre></div>
<p><img src="/BMMB554/img/lecture14/output_146_0.png" alt="png" /></p>

<blockquote>
<p>We have studied how a probability model has a distribution we call this $F$. $F$ often depends on parameters, which are denoted by Greek letters, such as
$\theta$. The observed data are generated via the brown arrow and are represented by Roman letters such as $x$. The vertical bar in the probability computation stands for supposing that or conditional on.</p>
</blockquote>

<h2 id="1-5-summary-of-this-chapter">1.5. Summary of this chapter</h2>

<p>We have used mathematical formulæ and Python to compute probabilities of various discrete events that can we modeled with a few basic distributions: The Bernoulli distribution was our most basic building block – it is used to represent a single binary trial such as a coin flip. We can code the outcomes as 0 and 1. We call $p$ the probability of success (the 1 outcome).</p>

<p>The binomial distribution is used for the number of 1s in $n$ binary trial and we generate the probabilities of seeing $k$
successes using the Python <code>scipy.stats</code> function <code>binom</code>. We also saw that we could simulate an <code>n</code>  trial binomial using the function <code>binom.rvs</code>.</p>

<p>The Poisson distribution is most appropriate for cases when $p$ is small (the 1s are rare). It has only one parameter $\lambda$, and the Poisson distribution for $\lambda = np$ is approximately the same as the binomial distribution for $(n, p)$ if $p$ is small. We used the Poisson distribution to model the number of randomly occuring false positives in an assay that tested for epitopes along a sequence, presuming that the per-position false positive rate $p$
 was small. We saw how such a parametric model enabled us to compute the probabilities of extreme events, as long as we knew all the parameters.</p>

<p>The multinomial distribution is used for discrete events that have more than two possible outcomes or levels. The power example showed us how to use Monte Carlo simulations to decide how much data we need to collect if we want to test whether a multinomial model with equal probabilities is consistent with the data. We used probability distributions and probabilistic models to evaluate hypotheses about how our data were generated, by making assumptions about the generative models. We term the probability of seeing the data, given a hypothesis, a $p$-value. This is not the same as the probability that the hypothesis is true!</p>

<h2 id="1-6-further-reading">1.6. Further reading</h2>

<ul>
<li>The elementary book by Freedman, Pisani, and Purves (1997) provides the best introduction to probability through the type of box models we mention here (Freedman, David, Robert Pisani, and Roger Purves. 1997. Statistics. New York, NY: WW Norton.).</li>
<li>The book by Durbin et al. (1998) covers many useful probability distributions and provides in its appendices a more complete view of the theoretical background in probability theory and its applications to sequences in biology. (Durbin, Richard, Sean Eddy, Anders Krogh, and Graeme Mitchison. 1998. Biological Sequence Analysis. Cambridge University Press.)</li>
<li>Chapter 6 will cover the subject of hypothesis testing. We also suggest Rice (2006) for more advanced material useful for the type of more advanced probability distributions, beta, gamma, exponentials we often use in data analyses. (Rice, John. 2006. Mathematical Statistics and Data Analysis. Cengage Learning.)</li>
</ul>



      </main>
    </div>
    <footer id="footer" class="mt-auto text-center text-muted">
  <div class="container">
    Nekrutenko Lab 2019 | CC-BY
  </div>
</footer>
    <script src="https://nekrut.github.io/BMMB554/js/feather.min.js"></script>
<script>
  feather.replace()
</script>


    
  

  </body>
</html>