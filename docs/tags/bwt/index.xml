<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bwt on BMMB554</title>
    <link>https://nekrut.github.io/BMMB554/tags/bwt/</link>
    <description>Recent content in Bwt on BMMB554</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 25 Feb 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://nekrut.github.io/BMMB554/tags/bwt/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Lecture 12: Mapping many reads quickly</title>
      <link>https://nekrut.github.io/BMMB554/lecture12/</link>
      <pubDate>Mon, 25 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://nekrut.github.io/BMMB554/lecture12/</guid>
      <description>Speeding things up The topics we discussed in the past lecture explain fundamental concepts behind analysis of biological sequences. Today, we will be talking about algorithms that allow aligning billions of sequencing reads against reference genomes. Similarly to the previous lecture I have borrowed heavily from the course taught by Ben Langmead at Johns Hopkins. The cover image is from Wikpedia article on Burrows-Wheeler transform.
The challenge of really large datasets In the previous lecture we have seen how dynamic programming helps aligning sequences.</description>
    </item>
    
  </channel>
</rss>